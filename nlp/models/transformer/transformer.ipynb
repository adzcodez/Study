{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('mlenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5ac7084b23d657c509653f1029e1595375871d367d601d901972afa788c0ca76"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Transformer\n",
    "\n",
    "This notebook accompanies the interview guide section on transformers. All code in this notebook is taken from [this](http://peterbloem.nl/blog/transformers) blogpost. \n",
    "\n",
    "## 1.1 Basic Self-Attention\n",
    "\n",
    "This is an implementation of basic self-attention. The input sequence of $t$ vectors with $k$ dimensions is a $t \\times k$ matrix $X$, and adding a minibatch gives us an input tensor of size $(b,t,k)$. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as functional\n",
    "\n",
    "b, t, k = 2, 5, 5\n",
    "shape = torch.ones(b, t, k) # 1s in the shape b, t, k\n",
    "X = torch.rand_like(shape)  # A tensor of random values in the shape b, t, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0.3274, 0.6003, 0.6281, 0.5550, 0.4303],\n",
       "         [0.5115, 0.6889, 0.6213, 0.4433, 0.5701],\n",
       "         [0.3373, 0.5820, 0.6288, 0.5632, 0.4240],\n",
       "         [0.3676, 0.6598, 0.6372, 0.4924, 0.4809],\n",
       "         [0.5098, 0.5641, 0.6574, 0.4597, 0.4309]],\n",
       "\n",
       "        [[0.3754, 0.4600, 0.2371, 0.5499, 0.5918],\n",
       "         [0.3478, 0.4873, 0.1995, 0.5780, 0.6767],\n",
       "         [0.3605, 0.4291, 0.2505, 0.5127, 0.5849],\n",
       "         [0.4223, 0.4616, 0.2391, 0.6293, 0.6593],\n",
       "         [0.3630, 0.4479, 0.2237, 0.5450, 0.6269]]])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "raw_weights = torch.bmm(X, X.transpose(1, 2)) # Multiply X by its transpose\n",
    "weights = F.softmax(raw_weights, dim=2) # Apply row-wise softmax to turn raw weights w'_ij into positive values that sum to 1\n",
    "y = torch.bmm(weights, X) # Multiply weight matrix by X to compute output sequence Y\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "## 1.2 Improved Self-Attention\n",
    "\n",
    "### Keys, Queries, Values\n",
    "Self-attention can go a lot further. Above, we can see that every input vector $x_i$ is used three times: \n",
    "*  Compared to every other vector to establish weights for its own output $y_i$, $w_{ij}$ \n",
    "*  Compared to every other vector to establish weights for the output of the $j^{th}$ vector $y_j$\n",
    "*  Used as part of the weighted sum to compute each output vector once weights are established\n",
    "\n",
    "But repeating these computations is very inefficient. The three roles above are, in order, query, key and value. We can trade off computation for space by creating new vectors for each role, rather than each vector playing all three. We add three $k \\times k$ weight matrices $W_q$, $W_k$ and $W_v$, and compute three linear transformations of each $x_i$ for the three different parts of the self attention. \n",
    "\n",
    "### Scaling the dot product\n",
    "Softmax is sensitive to large input values which kill the gradient and slow learning. As the average value of dot product will increase with the embedding dimension $k$, the dot product is scaled back to stop inputs from growing too large. \n",
    "\n",
    "### Multihead attention\n",
    "Basic self-attention is permutation equivariant. We can give greater discriminatory power by combining self-attention mechanisms with different weight matrices, called attention heads. Look at the interview guide for a better explanation. \n",
    "\n",
    "### Implementation \n",
    "Below is multi-head, scaled dot-product self attention using queries, keys and values. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=8):\n",
    "        super().__init__()\n",
    "        self.k, self.heads = k, heads\n",
    "\n",
    "        # Produces a concatenated vector of queries, keys and values for all heads\n",
    "        self.tokeys    = nn.Linear(k, k * heads, bias=False)\n",
    "        self.toqueries = nn.Linear(k, k * heads, bias=False)\n",
    "        self.tovalues  = nn.Linear(k, k * heads, bias=False)\n",
    "\n",
    "        # Unifies the outputs of the heads into a single k-vector \n",
    "        self.unifyheads = nn.Linear(heads * k, k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, k = x.size()\n",
    "        h = self.heads \n",
    "\n",
    "        queries = self.toqueries(x).view(b, t, h, k) # Reshape to b, t, h, k to give each head its own dimension\n",
    "        keys    = self.tokeys(x).view(b, t, h, k)\n",
    "        values  = self.tovalues(x).view(b, t, h, k)\n",
    "\n",
    "        # Fold heads into batch dimension to compute dot products\n",
    "        # Transpose first to get head and batch dimension next to each other (costly but unavoidable)\n",
    "        keys    = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "        values  = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "\n",
    "        ## Scale keys by 4root of k to save memory \n",
    "        queries = queries / (k ** (1/4))\n",
    "        keys    = keys / (k ** (1/4))\n",
    "\n",
    "        # Get dot product of queries and keys, then scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2)) # Size (b * h, t, t) and contains raw weights\n",
    "        dot = F.softmax(dot, dim=2) # Normalise row-wise\n",
    "\n",
    "        out = torch.bmm(dot, values).view(b, h, t, k) # Apply self attention to values\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * k) # Swap h and t back, and unify the heads\n",
    "        return self.unifyheads(out)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 1.3 Transformer Block\n",
    "Having built self-attention, we can now define a transformer block that incorporates this. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(k, heads=heads)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(k)\n",
    "        self.norm2 = nn.LayerNorm(k)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(k, 4 * k), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(4 * k, k)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "        fedforward = self.ff(x)\n",
    "\n",
    "        return self.norm2(fedforward + x)"
   ]
  },
  {
   "source": [
    "## 1.4 Sentiment Classification\n",
    "This uses the IMDb reviews dataset and the transformer to classify reviews as positive or negative. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_context'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-244498fd5851>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0m_context\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhere\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_context'"
     ]
    }
   ],
   "source": [
    "from _context import former\n",
    "from former import util\n",
    "\n",
    "from util import d, here\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random, tqdm, sys, math, gzip\n",
    "\n",
    "# Used for converting between nats and bits\n",
    "LOG2E = math.log2(math.e)\n",
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "NUM_CLS = 2\n",
    "\n",
    "def go(arg):\n",
    "    \"\"\"\n",
    "    Creates and trains a basic transformer for the IMDB sentiment classification task.\n",
    "    \"\"\"\n",
    "    tbw = SummaryWriter(log_dir=arg.tb_dir) # Tensorboard logging\n",
    "\n",
    "    # load the IMDB data\n",
    "    if arg.final:\n",
    "        train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "        TEXT.build_vocab(train, max_size=arg.vocab_size - 2)\n",
    "        LABEL.build_vocab(train)\n",
    "\n",
    "        train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=arg.batch_size, device=util.d())\n",
    "    else:\n",
    "        tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "        train, test = tdata.split(split_ratio=0.8)\n",
    "\n",
    "        TEXT.build_vocab(train, max_size=arg.vocab_size - 2) # - 2 to make space for <unk> and <pad>\n",
    "        LABEL.build_vocab(train)\n",
    "\n",
    "        train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=arg.batch_size, device=util.d())\n",
    "\n",
    "    print(f'- nr. of training examples {len(train_iter)}')\n",
    "    print(f'- nr. of {\"test\" if arg.final else \"validation\"} examples {len(test_iter)}')\n",
    "\n",
    "    if arg.max_length < 0:\n",
    "        mx = max([input.text[0].size(1) for input in train_iter])\n",
    "        mx = mx * 2\n",
    "        print(f'- maximum sequence length: {mx}')\n",
    "    else:\n",
    "        mx = arg.max_length\n",
    "\n",
    "    # create the model\n",
    "    model = former.CTransformer(emb=arg.embedding_size, heads=arg.num_heads, depth=arg.depth, seq_length=mx, num_tokens=arg.vocab_size, num_classes=NUM_CLS, max_pool=arg.max_pool)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    opt = torch.optim.Adam(lr=arg.lr, params=model.parameters())\n",
    "    sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (arg.lr_warmup / arg.batch_size), 1.0))\n",
    "\n",
    "    # training loop\n",
    "    seen = 0\n",
    "    for e in range(arg.num_epochs):\n",
    "\n",
    "        print(f'\\n epoch {e}')\n",
    "        model.train(True)\n",
    "\n",
    "        for batch in tqdm.tqdm(train_iter):\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            input = batch.text[0]\n",
    "            label = batch.label - 1\n",
    "\n",
    "            if input.size(1) > mx:\n",
    "                input = input[:, :mx]\n",
    "            out = model(input)\n",
    "            loss = F.nll_loss(out, label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradients\n",
    "            # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
    "            if arg.gradient_clipping > 0.0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)\n",
    "\n",
    "            opt.step()\n",
    "            sch.step()\n",
    "\n",
    "            seen += input.size(0)\n",
    "            tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.train(False)\n",
    "            tot, cor= 0.0, 0.0\n",
    "\n",
    "            for batch in test_iter:\n",
    "\n",
    "                input = batch.text[0]\n",
    "                label = batch.label - 1\n",
    "\n",
    "                if input.size(1) > mx:\n",
    "                    input = input[:, :mx]\n",
    "                out = model(input).argmax(dim=1)\n",
    "\n",
    "                tot += float(input.size(0))\n",
    "                cor += float((label == out).sum().item())\n",
    "\n",
    "            acc = cor / tot\n",
    "            print(f'-- {\"test\" if arg.final else \"validation\"} accuracy {acc:.3}')\n",
    "            tbw.add_scalar('classification/test-loss', float(loss.item()), e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"-e\", \"--num-epochs\",\n",
    "                        dest=\"num_epochs\",\n",
    "                        help=\"Number of epochs.\",\n",
    "                        default=80, type=int)\n",
    "\n",
    "    parser.add_argument(\"-b\", \"--batch-size\",\n",
    "                        dest=\"batch_size\",\n",
    "                        help=\"The batch size.\",\n",
    "                        default=4, type=int)\n",
    "\n",
    "    parser.add_argument(\"-l\", \"--learn-rate\",\n",
    "                        dest=\"lr\",\n",
    "                        help=\"Learning rate\",\n",
    "                        default=0.0001, type=float)\n",
    "\n",
    "    parser.add_argument(\"-T\", \"--tb_dir\", dest=\"tb_dir\",\n",
    "                        help=\"Tensorboard logging directory\",\n",
    "                        default='./runs')\n",
    "\n",
    "    parser.add_argument(\"-f\", \"--final\", dest=\"final\",\n",
    "                        help=\"Whether to run on the real test set (if not included, the validation set is used).\",\n",
    "                        action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\"--max-pool\", dest=\"max_pool\",\n",
    "                        help=\"Use max pooling in the final classification layer.\",\n",
    "                        action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\"-E\", \"--embedding\", dest=\"embedding_size\",\n",
    "                        help=\"Size of the character embeddings.\",\n",
    "                        default=128, type=int)\n",
    "\n",
    "    parser.add_argument(\"-V\", \"--vocab-size\", dest=\"vocab_size\",\n",
    "                        help=\"Number of words in the vocabulary.\",\n",
    "                        default=50_000, type=int)\n",
    "\n",
    "    parser.add_argument(\"-M\", \"--max\", dest=\"max_length\",\n",
    "                        help=\"Max sequence length. Longer sequences are clipped (-1 for no limit).\",\n",
    "                        default=512, type=int)\n",
    "\n",
    "    parser.add_argument(\"-H\", \"--heads\", dest=\"num_heads\",\n",
    "                        help=\"Number of attention heads.\",\n",
    "                        default=8, type=int)\n",
    "\n",
    "    parser.add_argument(\"-d\", \"--depth\", dest=\"depth\",\n",
    "                        help=\"Depth of the network (nr. of self-attention layers)\",\n",
    "                        default=6, type=int)\n",
    "\n",
    "    parser.add_argument(\"-r\", \"--random-seed\",\n",
    "                        dest=\"seed\",\n",
    "                        help=\"RNG seed. Negative for random\",\n",
    "                        default=1, type=int)\n",
    "\n",
    "    parser.add_argument(\"--lr-warmup\",\n",
    "                        dest=\"lr_warmup\",\n",
    "                        help=\"Learning rate warmup.\",\n",
    "                        default=10_000, type=int)\n",
    "\n",
    "    parser.add_argument(\"--gradient-clipping\",\n",
    "                        dest=\"gradient_clipping\",\n",
    "                        help=\"Gradient clipping.\",\n",
    "                        default=1.0, type=float)\n",
    "\n",
    "    options = parser.parse_args()\n",
    "\n",
    "    print('OPTIONS ', options)\n",
    "\n",
    "    go(options)"
   ]
  },
  {
   "source": [
    "## 1.5 Text Generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking \n",
    "dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "indices = torch.triu_indices(t, t, offset=1)\n",
    "dot[:, indices[0], indices[1]] = float('-inf')\n",
    "\n",
    "dot = F.softmax(dot, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}