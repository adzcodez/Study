{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('mlenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5ac7084b23d657c509653f1029e1595375871d367d601d901972afa788c0ca76"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# [Decision Trees](https://www.youtube.com/watch?v=Bqi7EFFvNOg&list=PLqnslRFeH2Upcrywf-u2etjdxxkL8nl7E&index=9)\n",
    "A decision tree tries to build a tree to split our data with the best separation of classes, and is also the basis for the random forest model which is covered in random_forest.ipynb. \n",
    "\n",
    "In this example, say we want to predict whether a person is walking or taking the bus to work - we have two features, `Rain` and `Time` and the response `Walk` which is classified as `No` or `Yes`. To build a tree that splits this data, we have all of the samples in the root node, and ask a question such as if it is raining, and then split the samples accordingly. We can then further split these samples into more leaves. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Entropy \n",
    "We will use entropy as the criterion for how to split regions. This is a measurement of uncertainty. \n",
    "\n",
    "$E = -\\overset{K}{\\underset{k=1}{\\Sigma}}p(X)\\log_2 p(X)$\n",
    "\n",
    "$p(X) = \\frac{\\#x}{n}$\n",
    "\n",
    "The formula is the sum of $p(X)$ times the log of $p(X)$. So we are looking at the product of $p(X)$ and $\\log p(X)$ for each class. This entropy gives us a value between 0 and 1, where 1 is the worst case (a 50/50 split of classes so no prediction can be made) and 0 is the best case (the region contains only one class). \n",
    "\n",
    "Using this dataset as an example, we have ten samples where 5 are 0 and 5 are 1: \n",
    "\n",
    "$S = [0,0,0,0,0,1,1,1,1,1]$\n",
    "\n",
    "Our entropy is: \n",
    "\n",
    "$E = -0.5 \\cdot (-1) - 0.5 \\cdot (-1) = 1 $\n",
    "\n",
    "which is the worst case. \n",
    "\n",
    "### Information gain\n",
    "Information gain is a measurement of the reduction in entropy from splitting the regions. If we were to begin by splitting based on rain, we would have: \n",
    "\n",
    "$S = [0,0,0,0,0,1,1,1,1,1], S_1 = [0,0,1,1,1,1,1], S_2 = [0,0,0]$\n",
    "\n",
    "The formula for information gain is given as: \n",
    "\n",
    "$E(\\text{parent}) - [\\text{weighted average} \\cdot E(\\text{children})]$\n",
    "\n",
    "and in this example would look like: \n",
    "\n",
    "$E(S) - [(7/10) * E(S_1) + (3/10) * E(S_2)]$\n",
    "\n",
    "$1 - [(7/10) \\times 0.863 + (3/10) * 0] = 0.395$\n",
    "\n",
    "We can use the information gain to perform a greedy search (going over all possible features and all possible feature thresholds) and selecting the best feature and threshold. \n",
    "\n",
    "### Algorithm \n",
    "We are going to have two parts to our decision tree classifier: first, we will build the tree by starting with the full set of data, and creating subsets based on a greedy search of information gain, and build this tree with recursive binary splitting with some kind of stopping criteria to prevent growth. We will then use the tree to predict by traversing the tree, checking each feature and returning a classification upon reaching a terminal node. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 2"
   ]
  },
  {
   "source": [
    "def entropy(y):\n",
    "    ' Takes in a vector of class labels and outputs the entropy ' \n",
    "    hist = np.bincount(y) # Use a histogram to count number of occurrences of all labels\n",
    "    ps = hist / len(y)    # p(X)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0]) # Return entropy"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node: \n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *,value=None):\n",
    "        self.feature   = feature\n",
    "        self.threshold = threshold \n",
    "        self.left      = left \n",
    "        self.right     = right \n",
    "        self.value     = value\n",
    "\n",
    "    def is_leaf(self):\n",
    "        ' Determines if node is a leaf node (if it has value) '\n",
    "        return self.value is not None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree: \n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None): \n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.root = None \n",
    "\n",
    "    def fit(self, X, y): \n",
    "        ' Takes in training data x and labels y to grow tree '\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_features, X.shape[1])\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        ' Traverses tree to make a prediction '\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape  # Number of samples and features\n",
    "        n_labels = len(np.unique(y))     # Number of different labels\n",
    "\n",
    "        # Stopping criteria \n",
    "        if (depth >= self.max_depth \n",
    "            or n_labels == 1\n",
    "            or n_samples < self.min_samples_split): \n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # If not met stopping criteria\n",
    "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
    "\n",
    "        # Greedy search\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left  = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1) # We want only the left idxs but all of the features\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feat, best_thresh, left, right) # Returns a node with the best feature, best threshold, and the left and right children\n",
    "\n",
    "    def _most_common_label(self, y): \n",
    "        ' Takes a vector of class labels and returns the most common class '\n",
    "        counter = Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0] # Returns the most common label\n",
    "        return most_common\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        ' Calculate information gain for each feature '\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None \n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column) \n",
    "            for threshold in thresholds: \n",
    "                gain = self._information_gain(y, X_column, threshold) \n",
    "\n",
    "                if gain > best_gain: \n",
    "                    best_gain = gain \n",
    "                    split_idx = feat_idx \n",
    "                    split_thresh = threshold\n",
    "        \n",
    "        return split_idx, split_thresh \n",
    "\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        ' Computes information gain '\n",
    "        # Parent entropy \n",
    "        parent_entropy = entropy(y)\n",
    "\n",
    "        # Generate split \n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        # Weighted average of child entropies \n",
    "        n = len(y) \n",
    "        n_left, n_right = len(left_idxs), len(right_idxs) \n",
    "        e_left, e_right = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
    "        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right\n",
    "\n",
    "        # Return information gain\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs  = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _traverse_tree(self, x, node): \n",
    "        ' Gets a sample and a start node and traverses the tree '\n",
    "        if node.is_leaf():\n",
    "            return node.value # If we are at a leaf, we have a value \n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left) # Go to the left\n",
    "        \n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "source": [
    "### Using our decision tree\n",
    "We can use this implementation on the breast cancer dataset from SciKit-Learn. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy is: 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred): \n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true) \n",
    "    return accuracy \n",
    "\n",
    "data = datasets.load_breast_cancer() \n",
    "X = data.data \n",
    "y = data.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "tree = DecisionTree(max_depth=10)\n",
    "tree.fit(X_train, y_train) \n",
    "\n",
    "y_pred = tree.predict(X_test) \n",
    "acc = accuracy(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy is: {acc}\")"
   ]
  },
  {
   "source": [
    "### Extensions\n",
    "Would be good to have tree pruning\n",
    "\n",
    "Also want to be able to visualise the tree, see what it's doing rather than be a black box accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}